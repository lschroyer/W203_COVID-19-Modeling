---
title: "clean_script"
author: "Ryan Mitchell"
date: "4/8/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction here

Introduction. Is the introduction clear? Is the research question specific and well defined? Does the introduction motivate a specific concept to be measured and explain how it will be operationalized. Does it do a good job of preparing the reader to understand the model specifications?

```{r load packages, echo=FALSE, warning=FALSE, message=FALSE}
rm(list = ls())
library(dplyr)
library(ggplot2) 
library(purrr)
library(haven)
library(tidyverse)
# install.packages("DescTools")
library(DescTools)
library(magrittr)
library(kableExtra)
require(plotrix)
# install.packages("magick")
# install.packages("webshot")
library("magick")
library("webshot")
webshot::install_phantomjs()

#install.packages("openxlsx") 
library(openxlsx)
#install.packages("readxl")
library(readxl)
#nstall.packages("rapportools")
library(rapportools)
#install.packages("data.table")
library(data.table)

library(patchwork)
library(sandwich)
library(lmtest)

#install.packages("corrplot")
#install.packages("psych")
library(corrplot)
library(psych)
```

```{r read in policy data script, echo=FALSE, message=FALSE, warning=FALSE}
tab_names <- excel_sheets(path = "data/US_Covid_Policy_data.xlsx")

list_all <- lapply(tab_names, 
                   function(x) read_excel(path = "data/US_Covid_Policy_data.xlsx", 
                                                     sheet = x))
#Rename list elements
names(list_all) <- tab_names %>% 
    tolower() %>%
    gsub(pattern = "_", replacement = " ") %>% 
    str_to_title() %>%
    gsub(pattern = " ", replacement = ".") 

#Rename colums in the DF
for (df in 1:length(list_all)){
  name_qc <- names(list_all)[df] 
  #print(name_qc)
  if(name_qc %in% c("Stay.At.Home", "Unemployment.Benefits")){
    names(list_all[[df]]) <- list_all[[df]][1,]
    list_all[[df]] <- list_all[[df]][-1,]
  }
  
  names(list_all[[df]]) <- gsub(" ", "_", names(list_all[[df]]) %>% tolower())
}

# names(list_all$Stay.At.Home) <- list_all$Stay.At.Home[1,]
# list_all$Stay.At.Home <- list_all$Stay.At.Home[-1,]


#Join the separate sheets into one master sheet
policy_df_0 <- list_all$State.Characteristics %>%
  select(state, state_abbreviation, state_fips_code,
         population = population_2018,
         area_sq_mi = square_miles,
         population_density = population_density_per_square_mile) %>% 
  left_join(list_all$State.Of.Emergency %>% 
              select(state, state_of_emergency_declared = state_of_emergency_issued,), 
            by = "state") %>% 
  left_join(list_all$Masks %>% 
              select(state, begin_mask_mandate = public_face_mask_mandate, end_face_mask_mandate),
            by = "state") %>% 
  mutate(begin_mask_mandate = as.Date(begin_mask_mandate, origin = "1899-12-30"),
         end_face_mask_mandate = as.Date(end_face_mask_mandate, origin = "1899-12-30")) %>%  
  mutate(mask_mandate_flag = ifelse(year(begin_mask_mandate) > 2000, 1, 0)) %>% 
  left_join(list_all$Stay.At.Home %>% 
              select(state,
                     begin_stay_at_home = `stay_at_home/shelter_in_place`, 
                     end_stay_at_home = `end_stay_at_home/shelter_in_place`), 
            by = "state") %>% 
  mutate(begin_stay_at_home = as.Date(as.numeric(begin_stay_at_home), origin = "1899-12-30"),
         end_stay_at_home = as.Date(as.numeric(end_stay_at_home), origin = "1899-12-30")) %>% 
  mutate(stay_at_home_flag = ifelse(year(begin_stay_at_home) > 2000, 1, 0)) %>%   
  left_join(list_all$Business.Closures %>% 
              select(state, first_business_closures, first_business_reopening), 
            by = "state") %>% 
  left_join(list_all$Travel.Quarantines %>% 
            select(state, quarantine_mandate_for_some_travelers,
                   quarantine_mandate_for_all_travelers,
                   end_travel_quarantine_mandate = quarantine_mandate_ended), 
            by = "state") %>% 
  mutate(quarantine_mandate_for_some_travelers = as.Date(quarantine_mandate_for_some_travelers, origin = "1899-12-30"),
         quarantine_mandate_for_all_travelers = as.Date(quarantine_mandate_for_all_travelers, origin = "1899-12-30")) %>%  
  mutate(begin_travel_quarantine_mandate = ifelse(quarantine_mandate_for_some_travelers >
                                               quarantine_mandate_for_all_travelers,
                                               quarantine_mandate_for_some_travelers,
                                               quarantine_mandate_for_all_travelers),
         begin_travel_quarantine_mandate = as.Date(begin_travel_quarantine_mandate),
         end_travel_quarantine_mandate = as.Date(end_travel_quarantine_mandate, origin = "1899-12-30")) %>% 
  mutate(travel_quarantine_mandate_flag = ifelse(year(begin_travel_quarantine_mandate) > 2000, 1, 0)) %>%   
  select(-quarantine_mandate_for_some_travelers, -quarantine_mandate_for_all_travelers) %>% 
  left_join(list_all$Unemployment.Benefits %>% 
            select(state,
                   begin_increased_unemployment_benefits = extended_benefits_program_activated,
                   end_increased_unemployment_benefits = extended_benefits_program_deactivated,
                   increased_weekly_unemployment_insurance_amt_thru_jul31 =
                     "weekly_ui_maximum_amount_with_extra_stimulus_(through_july_31,_2020)_(dollars)"), 
            by = "state") %>% 
  mutate(begin_increased_unemployment_benefits = 
           as.Date(as.numeric(begin_increased_unemployment_benefits), 
                   origin = "1899-12-30"),
         end_increased_unemployment_benefits = 
           as.Date(as.numeric(end_increased_unemployment_benefits),
                   origin = "1899-12-30"),
         increased_weekly_unemployment_insurance_amt_thru_jul31 =
           as.numeric(increased_weekly_unemployment_insurance_amt_thru_jul31))
  

saveRDS(policy_df_0, file = "pwd/AggregatedPolicyData.rds")
```

```{r load census, county population, mobility, and covid data, echo=FALSE, warning=FALSE, message=FALSE}
#Read in Census Data
acs_with_overlays <- read_csv("data/ACSDP1Y2019.DP05_data_with_overlays_2021-03-18T145421.csv", 
                              skip = 1, 
                              col_names = TRUE)

names(acs_with_overlays) <-  names(acs_with_overlays) %>% 
    tolower() %>%
    str_to_title() %>%
    gsub(pattern = " ", replacement = ".") %>%
    gsub(pattern = "!!", replacement = ".") %>%
    gsub(pattern = "!", replacement = ".")  


US_mobility_data_0 <- read_csv("data/2020_US_Region_Mobility_Report.csv", 
#                              skip = 1, 
                              col_names = TRUE)

names(US_mobility_data_0) <-  names(US_mobility_data_0) %>% 
    tolower() %>%
    str_to_title() %>%
    gsub(pattern = " ", replacement = ".") #%>%
    #gsub(pattern = "!!", replacement = ".") #%>%
    #gsub(pattern = "!", replacement = ".")  


# Get the area data by state
#https://www2.census.gov/library/publications/2001/compendia/ccdb00/tabB1.pdf
#https://www.census.gov/library/publications/2001/compendia/ccdb00.html
US_population_by_area_0 <- read.xlsx("data/LND01_census_area.xlsx", 
#                              skip = 1, 
                              colNames = TRUE) %>% 
  select(Areaname,	Census_fips_code = STCOU, SqMi = LND110190D)

US_mobility_data <- US_mobility_data_0 %>% 
  left_join(US_population_by_area_0, by = "Census_fips_code") %>% 
  select(-Areaname)

# US_mobility_data_1 %>% select(Census_fips_code) %>%  unique() %>% View()

# Get the population data by state
# https://www.census.gov/data/datasets/time-series/demo/popest/2010s-counties-total.html#par_textimage_70769902
US_population_by_county_0 <- read_csv("data/co-est2019-alldata.csv", 
#                              skip = 1, 
                              col_names = TRUE) %>% #glimpse()
  # select(COUNTY) %>% unique()
  select(STNAME, CTYNAME, POPESTIMATE2019)

names(US_population_by_county_0) <-  names(US_population_by_county_0) %>% 
    tolower() %>%
    str_to_title()

US_population_by_county <- US_population_by_county_0 %>% 
  group_by(Stname) %>% 
  mutate(StatePop = sum(Popestimate2019)/2) %>% #there is also a state level entry
  ungroup() %>% 
  mutate(CountyPopPerc = Popestimate2019/StatePop) %>% 
  arrange(Stname, desc(CountyPopPerc)) 
          
# US_population_by_county %>% 
#    filter(Stname == "New York") %>% View()
   #summarise(PercSum = sum(CountyPopPerc_Reallocated)) 

# Check to see if the county names match the mobility data county names
US_population_by_county$PresentFlag <- 0
US_population_by_county$Sub_region_2 <- NA

for (i in unique(US_mobility_data$Sub_region_2)){
  US_population_by_county <- US_population_by_county %>% 
    mutate(PresentFlag = if_else(Ctyname %like% i, 1, PresentFlag),
           Sub_region_2 = ifelse(Ctyname %like% i, i, Sub_region_2))
}
US_population_by_county$PresentFlag <- replace_na(US_population_by_county$PresentFlag, 0) 
#US_population_by_county %>% count(PresentFlag)

#Join the mobility data, population data, and area data
US_mobility_data2 <- US_mobility_data %>% 
  dplyr::rename(Stname = Sub_region_1) %>%
  left_join(US_population_by_county %>% 
              select(Stname, 
                     Sub_region_2, 
                     CountyPopPerc,
                     CountyPop = Popestimate2019, 
                     StatePop), 
            by = c("Stname","Sub_region_2")) %>% 
  dplyr::rename(Countyname = Sub_region_2) %>%
  filter(Stname != Countyname)

#Check this date has all 50 states
stopifnot(US_mobility_data2 %>% 
  filter(Date == "2020-03-30") %>% 
  select(Stname) %>% unique() %>% nrow() == 50)

#Add pop dens
US_mobility_data2_highpopdens0 <- US_mobility_data2 %>% 
  mutate(county_pop_dens = CountyPop/SqMi)

# US_mobility_data %>% 
#   filter(Date == "2020-03-30") %>%
#   arrange(desc(county_pop_dens)) %>% 
#   View()

#ggplot(US_mobility_data2_highpopdens0 %>% 
#         filter(Date == "2020-03-30"), 
#       aes(log10(county_pop_dens))) +
#  geom_histogram(bins = 100)

#User input
pop_dens_filter <- 2000

#ggplot(US_mobility_data2_highpopdens0 %>% 
#         filter(Date == "2020-03-30", county_pop_dens < pop_dens_filter), 
#       aes((county_pop_dens))) +
#  geom_histogram(bins = 100)

#Flag for areas greater than the pop_dens_filter people per sqmi cuttoff
US_mobility_data2_highpopdens <- US_mobility_data2_highpopdens0 %>% 
    mutate(high_popdens_county_flag = ifelse(county_pop_dens > pop_dens_filter, 1, 0))


US_mobility_data2_county_perc_reallocated <- US_mobility_data2_highpopdens %>% 
  filter(Date == "2020-03-30") %>%
  select(Stname, Countyname, CountyPopPerc, high_popdens_county_flag, 
         StatePop, CountyPop) %>% 
  group_by(Stname) %>% 
  mutate(TotalStatePopAccountedFor = sum(CountyPopPerc, na.rm = T),
         CountyPopPerc_Reallocated = CountyPopPerc/TotalStatePopAccountedFor, 
         high_popdens_flag = max(high_popdens_county_flag, na.rm = T)) %>% 
  ungroup()


#QC that all reallocated perc weights totals to 100 
stopifnot(US_mobility_data2_county_perc_reallocated %>% 
  group_by(Stname) %>%
  summarise(PercSum = sum(CountyPopPerc_Reallocated)) %>% # View()
  filter(PercSum < 1.01 & PercSum > .99) %>% 
  nrow() == (US_mobility_data2_highpopdens$Stname %>% 
               unique() %>% length())) # Check no high pop states were dropped 

US_mobility_data2_perc_pop_highdens <- US_mobility_data2_county_perc_reallocated %>% 
  filter(high_popdens_county_flag == 1) %>% 
  group_by(Stname) %>% 
  summarise(high_popdens_pop_perc = sum(CountyPop, na.rm = T)/StatePop) %>% 
  ungroup() %>% 
  distinct(.keep_all = T) %>% 
  filter(!is.na(high_popdens_pop_perc))

#Join new percents and high pop density percents
US_mobility_data3 <- US_mobility_data2_highpopdens %>% 
  left_join(US_mobility_data2_county_perc_reallocated, 
            by = c("Stname", "Countyname")) %>% 
  left_join(US_mobility_data2_perc_pop_highdens, 
            by = "Stname")
  
#reset names
US_mobility_data <- US_mobility_data3
rm(US_mobility_data2, US_mobility_data3, US_population_by_county_0, 
   US_mobility_data2_highpopdens, US_mobility_data2_highpopdens0,
   US_population_by_county, US_population_by_area_0, US_mobility_data_0, 
   US_mobility_data2_county_perc_reallocated)


## Pull data from NYT COVID Database and plot summary
NYT_Data <- fread("https://raw.githubusercontent.com/nytimes/covid-19-data/master/us-states.csv")
#summary(NYT_Data)

## Convert date strings to dates. 
NYT_Data[,date:=as.Date(date)]
#summary(NYT_Data)

## Calculate cases on a time interval
start.date <- as.Date("2021-01-01")
end.date <- as.Date("2021-02-28")

interval.cases <- NYT_Data %>%
  arrange(state, 
          date == start.date) %>%
  group_by(state) %>%
  mutate(cases_inc = cases - lag(cases),
         deaths_inc = deaths - lag(deaths)) %>%
  ungroup() %>% 
  filter(cases_inc >= 0)

```

```{r join mobility and covid, echo=FALSE, message=FALSE, warning=FALSE}
#join mobility data with covid database

US_mobility_data_agg <- US_mobility_data %>% 
  dplyr::rename("state" = "Stname",
         "date" = "Date") %>%
  group_by(state, date, high_popdens_pop_perc) %>% 
  mutate(Retail_and_recreation_percent_change_from_baseline_weighted = 
           Retail_and_recreation_percent_change_from_baseline * CountyPopPerc_Reallocated,
         Grocery_and_pharmacy_percent_change_from_baseline_weighted = 
           Grocery_and_pharmacy_percent_change_from_baseline * CountyPopPerc_Reallocated,
         Parks_percent_change_from_baseline_weighted = 
           Parks_percent_change_from_baseline * CountyPopPerc_Reallocated,
         Transit_stations_percent_change_from_baseline_weighted =
           Transit_stations_percent_change_from_baseline * CountyPopPerc_Reallocated,
         Workplaces_percent_change_from_baseline_weighted = 
           Workplaces_percent_change_from_baseline * CountyPopPerc_Reallocated,
         Residential_percent_change_from_baseline_weighted = 
           Residential_percent_change_from_baseline * CountyPopPerc_Reallocated) %>% 
  summarize(Retail_and_recreation_percent_change_from_baseline_weighted = 
              sum(Retail_and_recreation_percent_change_from_baseline_weighted, na.rm = T),
            Grocery_and_pharmacy_percent_change_from_baseline_weighted = 
              sum(Grocery_and_pharmacy_percent_change_from_baseline_weighted, na.rm = T),
            Parks_percent_change_from_baseline_weighted = 
              sum(Parks_percent_change_from_baseline_weighted, na.rm = T),
            Transit_stations_percent_change_from_baseline_weighted = 
              sum(Transit_stations_percent_change_from_baseline_weighted, na.rm = T), 
            Workplaces_percent_change_from_baseline_weighted = 
              sum(Workplaces_percent_change_from_baseline_weighted, na.rm = T),
            Residential_percent_change_from_baseline_weighted = 
              sum(Residential_percent_change_from_baseline_weighted, na.rm = T)) %>% 
  ungroup()

US_cases_and_mobility <- US_mobility_data_agg %>% 
  left_join(interval.cases, by = c("state", "date"))

#QC the join
stopifnot(US_cases_and_mobility %>% nrow()
  == US_mobility_data_agg %>% nrow())

#US_cases_and_mobility %>% 
#  filter(is.na(cases_inc),
#         !is.na(state)) %>% 
#  select(state, cases_inc) %>% nrow()
#1000

#US_cases_and_mobility %>% nrow()
#19800

#1000/19800*100
#[1] 5.1% missing inc cases data
```

# The Initial Data Loading and Cleaning. 

Did the team notice any anomalous values? Is there a sufficient justification for any data points that are removed? Did the report note any coding features that affect the meaning of variables (e.g. top-coding or bottom-coding)? Overall, does the report demonstrate a thorough understanding of the data? Does the report convey this understand to its reader – can the reader, through reading this report, come to the same understanding that the team has come to?


```{r data wrangling and final df, echo=FALSE, message=FALSE, warning=FALSE}
#For each state, read in the first 365 days of transit change data, beginning at the date the state declared an emergency 
first_365 <- US_cases_and_mobility %>% 
  left_join(policy_df_0, by = c("state")) %>%
  filter(as.Date(date) >= as.Date(state_of_emergency_declared) & 
           as.Date(date) <= as.Date(state_of_emergency_declared) + 364)
  

#Calculate the median county-population-weigthed value of transit changes for each state in the first 365 days after emergency declaration
transit_chg <- aggregate(first_365$Transit_stations_percent_change_from_baseline_weighted, by=list(state=first_365$state), FUN=median)  %>%
  dplyr::rename(transit = x)

retail_chg <- aggregate(first_365$Retail_and_recreation_percent_change_from_baseline_weighted,by=list(state=first_365$state), FUN=median) %>%
  dplyr::rename(retail = x)

residential_chg <- aggregate(first_365$Residential_percent_change_from_baseline_weighted, by=list(state=first_365$state), FUN=median) %>%
  dplyr::rename(residential = x)

grocery_chg <- aggregate(first_365$Grocery_and_pharmacy_percent_change_from_baseline_weighted, by=list(state=first_365$state), FUN=median) %>%
  dplyr::rename(grocery = x)

parks_chg <- aggregate(first_365$Parks_percent_change_from_baseline_weighted, by=list(state=first_365$state), FUN=median)  %>%
  dplyr::rename(parks = x)

workplace_chg <- aggregate(first_365$Workplaces_percent_change_from_baseline_weighted, by=list(state=first_365$state), FUN=median)  %>%
  dplyr::rename(workplace = x)

z <- data.frame(transit_chg$transit, retail_chg$retail, grocery_chg$grocery, parks_chg$parks, workplace_chg$workplace, residential_chg$residential) %>%
  dplyr::rename(Transit = transit_chg.transit, Retail = retail_chg.retail, Grocery = grocery_chg.grocery,
                Parks = parks_chg.parks, Workplace = workplace_chg.workplace, Residential = residential_chg.residential)

#Visualize the correlation between mobility changes
M <- cor(z)
mat1 <- data.matrix(M)
print(M)
corrplot(mat1, method = "color", tl.col = 'black', is.corr=FALSE, title = 'Correlation Matrix: Median Mobility Changes by Category')

#Very high correlations between the mobility changes, except for Parks. We will first explore whether parks mobility change demonstrates a clear relationship with COVID cases, and then explore whether transit mobility shows a linear relationship with cases.

#Calculate the median county-population-weighted value of transit changes for each state in the first 365 days after emergency declaration
mobility_chg <- aggregate(first_365$Transit_stations_percent_change_from_baseline_weighted, by=list(state=first_365$state), FUN=median) %>%
  dplyr::rename(median_transit_change = x)


#Calculate the cumulative cases at 365 days post emergency declaration by state  
cases_at_365 <- aggregate(first_365$cases, by=list(state=first_365$state), FUN=max) %>%
  dplyr::rename(cum_cases_at_365d = x)

#Grab the % of population living in high density variable
high_dens <- aggregate(first_365$high_popdens_pop_perc, by=list(state=first_365$state), FUN=mean) %>%
  dplyr::rename(high_popdens_pop_perc = x)

#Replace NaN with 0
high_dens$high_popdens_pop_perc[is.nan(high_dens$high_popdens_pop_perc)]<-0

#Create our final dataframe!
final_df <- cases_at_365 %>%
  left_join(policy_df_0, by = c("state")) %>%
  left_join(parks_chg, by = c("state")) %>%
  left_join(mobility_chg, by = c("state")) %>%
  left_join(high_dens, by = c("state")) %>%
  mutate(cases_per_100k_at_365d = 100000*cum_cases_at_365d/population) %>%
  left_join(acs_with_overlays, by = c("state" = "Geographic.Area.Name"))


#Relationship between median parks mobility change and cumulative case count per 100k at 365 days post statement of emergency
ggplot(data = final_df) +
  geom_point(aes(x = parks,
                 y = cases_per_100k_at_365d)) +
  geom_smooth(aes(x = parks,
                 y = cases_per_100k_at_365d),
            colour = 'red',
            method = "lm", se = FALSE) + 
  geom_smooth(aes(x = parks,
                  y = cases_per_100k_at_365d),
            colour = 'blue',
            method = "loess", se = TRUE) + 
  labs(title = 'State-Level One Year Median Parks Mobility Change vs. 
       Cases Per 100K People at One Year After SOE', x='Median Daily Relative % Parks Mobility Change', y='Cases Per 100K People One Year After SOE') 


#Relationship between median transit change and cumulative case count per 100k at 365 days post statement of emergency
ggplot(data = final_df) +
  geom_point(aes(x = median_transit_change,
                 y = cases_per_100k_at_365d)) +
  geom_smooth(aes(x = median_transit_change,
                 y = cases_per_100k_at_365d),
            colour = 'red',
            method = "lm", se = FALSE) + 
  geom_smooth(aes(x = median_transit_change,
                  y = cases_per_100k_at_365d),
            colour = 'blue',
            method = "loess", se = TRUE) + 
  labs(title = 'State-Level One Year Median Transit Mobility Change vs. 
       Cases Per 100K People at One Year After SOE', x='Median Daily Relative % Transit Mobility Change', y='Cases Per 100K People One Year After SOE') 

#Assess whether to use mean or median change in transit: We chose median because there is some skew in the data
h <- hist(first_365$Transit_stations_percent_change_from_baseline_weighted, 
     main = paste("Histogram of Relative Daily Changes in Transit Mobility for US States:
                  First Year"),  
     xlab='Relative % Change From Baseline Mobility', ylab='Frequency (Days)', col='gray') 

#h$density = h$density = h$counts/sum(h$counts)
#plot(h, freq = FALSE, col='gray', main = paste("Histogram of Relative Daily Changes in Transit Mobility for US States:
#                  First Year"), xlab='Relative % Change From Baseline Mobility', ylab='Frequency (Days)')


#Simple regression model on the relationship between change in parks mobility and COVID cases per 100K people. 
model_p <- lm(cases_per_100k_at_365d ~ parks, data = final_df)

#Pull out the p-values for the simple regression coefficients
summary(model_p)$coefficients[,4]
```

After inspection of the correlation matrix between the different measures of mobility changes, we chose to use the state-level median change in transit mobility in the 365 days after each state declared an emergency as our main variable of interest for model 1. Our motivation for choosing transit over the other mobility features was as follows:

1) Transit is most closely aligned with our understanding of how viruses spread, particularly from one locality or population center to another. 

2) The mobility changes in Transit, Retail, Grocery, and Workplace are highly positively correlated with each other (>0.8 in each pair), and highly negatively correlated with Residential mobility changes (absolute value of >0.7 or above). Highly correlated features should be avoided in descriptive and explanatory linear regression modeling as they tend to increase the standard error estimates on the model parameter estimates for the correlated features. 

The only mobility metric that was not strongly correlated with the others was Parks. Since the median change in Park mobility was not strongly correlated with any other mobility changes, we examined its relationship with our target variable but did not observe a positive or negative linear relationship. A t-Test on our calculated simple regression coefficient failed to reject the null hypothesis that there was no evidence that the coefficient for change in median Parks mobility was measurably different than zero. 

3) We did not want to take an aggregation of the set of highly correlated features (Transit, Retail, Grocery, and Workplace) because the raw data provided are relative numbers and we do not have access to the underlying absolute mobility data, so we would be unable to derive a correct weighted average of these features.

4) Given that the distribution of relative transit mobility change had a left skew, our team decided to use the median value for each state within the 365 day window as a better measure of central tenancy. 


# The Model Building Process. 

Overall, is each step in the model building process supported by EDA? Is the outcome variable appropriate? Did the team clearly state why they chose these explanatory variables, does this explanation make sense in term of their research question? Did the team consider available variable transformations and select them with an eye towards model plausibility and interpretability? Are transformations used to expose linear relationships in scatterplots? Is there enough explanation in the text to understand the meaning of each visualization?


# Base Model. 

Does this model only include key explanatory variables? Do the variables make sense given the measurement goals? Did the team apply reasonable transformations to these variables, to capture the nature of the relationships? Does the team write about this model in prose in a way that is appropriate?

```{r model, echo=FALSE, message=FALSE}
model_1 <- lm(cases_per_100k_at_365d ~ median_transit_change
              , data = final_df)

qplot(model_1$residuals,
               geom = "histogram", bins = 20) +
         labs(title = "Histogram of residuals",
              x = "residual")

qplot(model_1$fitted, model_1$residuals,
            geom = "point") +
         geom_abline(intercept = 0,
                     slope = 0,
                     colour = "red") + 
         geom_smooth(aes(x = model_1$fitted,
                         y = model_1$residuals),
            colour = 'blue',
            method = "loess", se = TRUE) +
         labs(title = "Plot of residuals vs fitted values",
              x = "fitted value",
              y = "residual")

cat("-------Coef-------\n")
coefficients(model_1)

cat("\n-------Model results-------\n")
summary(model_1)
cat("\n")


cat("\n-------Model results-------\n")
summary(model_p)
cat("\n")
```

# Model 1 Discussion Here:

# Second Model. 

Does this model represent a balanced approach, including variables that advance modeling goals without causing major issues? Does the model succeed in reducing standard errors of the key variables compared to the base model? Does it capture major non-linearities in the joint distribution of the variables? Does the team write about this model in prose in a way that is appropriate?

# EDA for Model 2:

One of the contributing factors to the spread of COVID-19 is asymptomatic spread. Younger people have been shown to have milder symptoms and therefore it stands to reason that they may be less likely to get tested, and ultimately end up spreading the disease at a greater rate than older age groups. To make matters worse, younger people tend to interact with more people (source?) as a result of being in school (switching between classrooms, touching desks and other surfaces where other students have been, congregating in large cafeterias, etc.), which increases the probability of viral transmission. Hence, for our second model, we were interested in testing our general hypothesis that age demographics may play a role in the spread of COVID-19. We began by doing some basic exploratory data analysis (EDA) with respect to age distributions and cumulative COVID-19 case counts per 100K, which is our target variable for Lab 2.

```{r age EDA < 24, echo=FALSE, message=FALSE}
census_df<-final_df[c('cases_per_100k_at_365d', 'Percent.Sex.And.Age.Total.Population.Under.5.Years', 'Percent.Sex.And.Age.Total.Population.5.To.9.Years', 'Percent.Sex.And.Age.Total.Population.10.To.14.Years', 'Percent.Sex.And.Age.Total.Population.15.To.19.Years','Percent.Sex.And.Age.Total.Population.20.To.24.Years')]

names(census_df) <- names(census_df) %>% gsub(pattern = "Percent.Sex.And.Age.Total.Population.", replacement = "Pct. ") %>%  gsub(pattern = ".To.", replacement = "- \n") %>%  gsub(pattern = ".Years", replacement = " Yrs") %>%  gsub(pattern = 'cases_per_100k_at_365d', replacement = " Cases per \n 100k")

panel.cor<-function(x,y)
  {
    usr<-par("usr"); on.exit(par(usr))
    par(usr=c(0,1,0,1))
    r=round(cor(x,y),digits=2)
    text(0.5,0.5,r)
  }

pairs(census_df,lower.panel = panel.cor,upper.panel = panel.smooth)
```


```{r age EDA 25-59, echo=FALSE, message=FALSE}
census_df<-final_df[c('cases_per_100k_at_365d', 'Percent.Sex.And.Age.Total.Population.25.To.34.Years','Percent.Sex.And.Age.Total.Population.35.To.44.Years','Percent.Sex.And.Age.Total.Population.45.To.54.Years', 'Percent.Sex.And.Age.Total.Population.55.To.59.Years')]

names(census_df) <- names(census_df) %>% gsub(pattern = "Percent.Sex.And.Age.Total.Population.", replacement = "Pct. ") %>%  gsub(pattern = ".To.", replacement = "- \n") %>%  gsub(pattern = ".Years", replacement = " Yrs") %>%  gsub(pattern = 'cases_per_100k_at_365d', replacement = " Cases per \n 100k")

panel.cor<-function(x,y)
  {
    usr<-par("usr"); on.exit(par(usr))
    par(usr=c(0,1,0,1))
    r=round(cor(x,y),digits=2)
    text(0.5,0.5,r)
  }

pairs(census_df,lower.panel = panel.cor,upper.panel = panel.smooth)
```

```{r age EDA 60+, echo=FALSE, message=FALSE}
census_df<-final_df[c('cases_per_100k_at_365d','Percent.Sex.And.Age.Total.Population.60.To.64.Years','Percent.Sex.And.Age.Total.Population.65.To.74.Years', 'Percent.Sex.And.Age.Total.Population.75.To.84.Years', 'Percent.Sex.And.Age.Total.Population.85.Years.And.Over')]

names(census_df) <- names(census_df) %>% gsub(pattern = "Percent.Sex.And.Age.Total.Population.", replacement = "Pct. ") %>%  gsub(pattern = ".To.", replacement = "- \n") %>%  gsub(pattern = ".Years", replacement = " Yrs") %>%  gsub(pattern = 'cases_per_100k_at_365d', replacement = " Cases per \n 100k")

panel.cor<-function(x,y)
  {
    usr<-par("usr"); on.exit(par(usr))
    par(usr=c(0,1,0,1))
    r=round(cor(x,y),digits=2)
    text(0.5,0.5,r)
  }

pairs(census_df,lower.panel = panel.cor,upper.panel = panel.smooth)
```


```{r age demo F-tests for feature creation, echo=FALSE, message=FALSE}

#Build nested models for categorical age demographic data where visual inspection suggested interesting relationships with target
intercept_model = lm(cases_per_100k_at_365d ~ 1, data = final_df)

model_a = lm(cases_per_100k_at_365d ~ Percent.Sex.And.Age.Total.Population.Under.5.Years, data = final_df)

model_b = lm(cases_per_100k_at_365d ~ Percent.Sex.And.Age.Total.Population.Under.5.Years + Percent.Sex.And.Age.Total.Population.5.To.9.Years, data = final_df)

model_c = lm(cases_per_100k_at_365d ~ Percent.Sex.And.Age.Total.Population.Under.5.Years + Percent.Sex.And.Age.Total.Population.5.To.9.Years + Percent.Sex.And.Age.Total.Population.10.To.14.Years, data = final_df)

model_d = lm(cases_per_100k_at_365d ~ Percent.Sex.And.Age.Total.Population.Under.5.Years + Percent.Sex.And.Age.Total.Population.5.To.9.Years + Percent.Sex.And.Age.Total.Population.10.To.14.Years + Percent.Sex.And.Age.Total.Population.15.To.19.Years, data = final_df)

model_e = lm(cases_per_100k_at_365d ~ Percent.Sex.And.Age.Total.Population.Under.5.Years + Percent.Sex.And.Age.Total.Population.5.To.9.Years + Percent.Sex.And.Age.Total.Population.10.To.14.Years + Percent.Sex.And.Age.Total.Population.15.To.19.Years + Percent.Sex.And.Age.Total.Population.20.To.24.Years, data = final_df)

#Significant
anova(model_a, intercept_model, test = "F")

#Insignificant 
anova(model_b, model_a, test = "F")

#Significant F-test
anova(model_c, model_a, test = "F")

#Insignificant relative to model_c
anova(model_d, model_c, test = "F")
anova(model_e, model_d, test = "F")
anova(model_e, model_c, test = "F")
```

When performing EDA on the categorical age group distributions and their relationship to our target variable, we noticed a strong positive correlation to the target among age groups 0-24. Ages groups in the 24+ range did not appear to follow a consistent pattern with respect to correlation with our target variable and were therefore not a key focus area for our analysis.

Next, we performed a series of F-tests on a nested set of linear regression models with parameter estimates for this set of age groups (Under 5, 5-9, 10-14, 15-19, and 20-24). Our motivation here was to understand whether the regression residuals were measurably different from one another between the different (nested) model specifications. For context, the null hypothesis for an F-test is that fitting additional coefficients for a longer model does not measurably reduce the residuals relative to a nested model with fewer parameters. 

Our first F-test compared a simple intercept-only estimator model with a model that had a parameter estimate for the percentage of the population under 5 years old. With a p-value of 0.0002, we reject the null hypothesis and use this model (model_a) as the baseline model for subsequent comparisons with additional parameter estimates for different age categories. 

We failed to reject the null hypothesis when adding an estimator for ages 5-9 (model_b), and succeeded in rejecting the null hypothesis when adding an estimator for ages 10-14 (model_c), with a p-value of 0.0001. Adding additional estimators for 15-19 and 20-24 failed to reject the null hypothesis that these models (model_d and model_e) were measurably better at reducing residuals than model_c. These results suggested creating features for ages 0-9 and 10-24 and adding them to our base model_1.

```{r df update 1, echo=FALSE, message=FALSE} 
final_df <- final_df %>% mutate(pop_pct_age_0_9 = Percent.Sex.And.Age.Total.Population.Under.5.Years + Percent.Sex.And.Age.Total.Population.5.To.9.Years,
                    pop_pct_age_10_24 = Percent.Sex.And.Age.Total.Population.10.To.14.Years + Percent.Sex.And.Age.Total.Population.15.To.19.Years + Percent.Sex.And.Age.Total.Population.20.To.24.Years)

cor(final_df$pop_pct_age_0_9, final_df$pop_pct_age_10_24)

final_df <- final_df %>% mutate(pop_pct_age_0_24 = Percent.Sex.And.Age.Total.Population.Under.5.Years + Percent.Sex.And.Age.Total.Population.5.To.9.Years + Percent.Sex.And.Age.Total.Population.10.To.14.Years + Percent.Sex.And.Age.Total.Population.15.To.19.Years + Percent.Sex.And.Age.Total.Population.20.To.24.Years)
```

However, after measuring a 0.77 correlation between these two features - and with the benefit of increased model parsimony - we decided to group them together to prevent the standard errors for their respective coefficients from increasing substantially. Let's take a look and assess whether this new variable visually satisfies the conditional linearity expectation with respect to our target.

```{r incremental variables 1, echo=FALSE, message=FALSE}
ggplot(data = final_df) +
  geom_point(aes(x = pop_pct_age_0_24,
                 y = cases_per_100k_at_365d)) +
  geom_smooth(aes(x = pop_pct_age_0_24,
                 y = cases_per_100k_at_365d),
            colour = 'red',
            method = "lm", se = FALSE) + 
  geom_smooth(aes(x = pop_pct_age_0_24,
                  y = cases_per_100k_at_365d),
            colour = 'blue',
            method = "loess", se = TRUE) + 
  labs(title = 'Percentage of State Population < 24 Years Old vs. 
       Cases Per 100K People at One Year After SOE', x='Percentage of State Population < 24 Years Old', y='Cases Per 100K People One Year After SOE') 
```

The conditional linear expectation between our population percentage aged 0-24 and our target appears to be met. In addition to evaluating the linear relationships between the demographic age variables and our targets (and with each-other), we also evaluated the log, square root, and square transformations to the age distribution data but did not find them to aid in reducing the frequency or magnitude of outlier data points. 


Next, we wanted to introduce a control variable for population density...

```{r anova}
#---Commented out because I collapsed 0-9 and 10-24 into 0-24
#final_df <- final_df %>% mutate(pop_pct_age_0_9 = Percent.Sex.And.Age.Total.Population.Under.5.Years + Percent.Sex.And.Age.Total.Population.5.To.9.Years,
#                    pop_pct_age_10_24 = Percent.Sex.And.Age.Total.Population.10.To.14.Years + Percent.Sex.And.Age.Total.Population.15.To.19.Years + #Percent.Sex.And.Age.Total.Population.20.To.24.Years)

#model_2a <- lm(cases_per_100k_at_365d ~ median_transit_change + pop_pct_age_0_9, data = final_df)
#model_2b <- lm(cases_per_100k_at_365d ~ median_transit_change + pop_pct_age_0_9 + pop_pct_age_10_24, data = final_df)
#anova(model_2a, model_1, test = "F")
#anova(model_2a, model_2b, test = "F")

model_2a <- lm(cases_per_100k_at_365d ~ median_transit_change + pop_pct_age_0_24, data = final_df)

# Significant
anova(model_2a, model_1, test = "F")


model_2b <- lm(cases_per_100k_at_365d ~ median_transit_change + pop_pct_age_0_24 + population_density, data = final_df)

cat("\n-------Model results-------\n")
summary(model_2b)
cat("\n")

# Significant
anova(model_2b, model_2a, test = "F")


# Visually Evaluate Model 2b
qplot(model_2b$residuals,
               geom = "histogram", bins = 20) +
         labs(title = "Histogram of residuals",
              x = "residual")

qplot(model_2b$fitted, model_2b$residuals,
            geom = "point") +
         geom_abline(intercept = 0,
                     slope = 0,
                     colour = "red") + 
         geom_smooth(aes(x = model_2b$fitted,
                         y = model_2b$residuals),
            colour = 'blue',
            method = "loess", se = TRUE) +
         labs(title = "Plot of residuals vs fitted values",
              x = "fitted value",
              y = "residual")

```
```{r m2}
cor(final_df$median_transit_change, final_df$pop_pct_age_0_24)
cor(final_df$median_transit_change, final_df$population_density)
```


# Third Model. 

Does this model represent a maximalist approach, erring on the side of including most variables? Is it still a reasonable model? Are there any variables that are outcomes, and should therefore still be excluded? Is there too much colinearity, to the point that the key causal effects cannot be measured? Does this team write about this model in prose in a way that is appropriate?


# A Regression Table. 

Are the model specifications properly chosen to outline the boundary of reasonable choices? Is it easy to find key coefficients in the regression table? Does the text include a discussion of practical significance for key effects?

# Plots, Figures, and Tables 

Do the plots, figures and tables that the team has chosen to include successfully move forward the argument that they are making? Has the team chosen the most effective method (a table or a chart) to display their evidence? Is that table or chart the most communicative it could be? Is every plot, figure, and table that is included in the report referenced in the narrative argument?

# Assessment of the CLM. 

Has the team presented a sober assessment of the CLM assumptions that might be problematic for their model? Have they presented their analysis about the consequences of these problems (including random sampling) for the models they estimate? Did they use visual tools or statistical tests, as appropriate? Did they respond appropriately to any violations?

# An Omitted Variables Discussion. 

Did the report miss any important sources of omitted variable bias? Are the estimated directions of bias correct? Was their explanation clear? Is the discussion connected to whether the key effects are real or whether they may be solely an artifact of omitted variable bias?

# Conclusion. 

Does the conclusion address the research question? Does it raise interesting points beyond numerical estimates? Does it place relevant context around the results?

Are there any other errors, faulty logic, unclear or unpersuasive writing, or other elements that leave you less convinced by the conclusions?

# General Notes:

"In principle the SE reflects the degree of uncertainty or the lack of information for getting a 'good' ( that is reliable) estimate of a parameter. Therefore if you keep everything else the same ( eg the same variation in the response, the same number of observations) but you increase the number of separate parameters to be estimated there will be less information per parameter to get the estimate, and hence larger standard error. Precisely what happens will depend on the the degree of variation in the additional X variable that is included and how colinear it is with already included variables."

Known IID violations:
Geo-spatial dependence (states near each other are not independent...physical proximity)
Policy coordination dependence (states near each-other have coordinated policies (like NY/NJ quarantine policies, etc))

Other limitations: 
Mobility data is based on Google-Maps cell phone users. Not everyone has access to a smart phone or uses Google Maps and allows their location to be traced, so this data may not be representative of the population. Additionally, we do not have absolute numbers, only relative change data. 
