---
title: "clean_script"
author: "Ryan Mitchell"
date: "4/8/2021"
output:
  html_document: default
  pdf_document: default

---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


# Introduction here

Introduction. Is the introduction clear? Is the research question specific and well defined? Does the introduction motivate a specific concept to be measured and explain how it will be operationalized. Does it do a good job of preparing the reader to understand the model specifications?

```{r load packages, echo=FALSE, warning=FALSE, message=FALSE}
rm(list = ls())
library(dplyr)
library(ggplot2) 
library(purrr)
library(haven)
library(tidyverse)
# install.packages("DescTools")
library(DescTools)
library(magrittr)
library(kableExtra)
require(plotrix)
# install.packages("magick")
# install.packages("webshot")
library("magick")
library("webshot")
webshot::install_phantomjs()

#install.packages("openxlsx") 
library(openxlsx)
#install.packages("readxl")
library(readxl)
#nstall.packages("rapportools")
library(rapportools)
#install.packages("data.table")
library(data.table)

library(patchwork)
library(sandwich)
library(lmtest)

#install.packages("corrplot")
#install.packages("psych")
library(corrplot)
library(psych)
library(stargazer)
```

# Initial Data Loading and Cleaning

```{r read in policy data script, echo=FALSE, message=FALSE, warning=FALSE}
tab_names <- excel_sheets(path = "data/US_Covid_Policy_data.xlsx")

list_all <- lapply(tab_names, 
                   function(x) read_excel(path = "data/US_Covid_Policy_data.xlsx", 
                                                     sheet = x))
#Rename list elements
names(list_all) <- tab_names %>% 
    tolower() %>%
    gsub(pattern = "_", replacement = " ") %>% 
    str_to_title() %>%
    gsub(pattern = " ", replacement = ".") 

#Rename colums in the DF
for (df in 1:length(list_all)){
  name_qc <- names(list_all)[df] 
  #print(name_qc)
  if(name_qc %in% c("Stay.At.Home", "Unemployment.Benefits")){
    names(list_all[[df]]) <- list_all[[df]][1,]
    list_all[[df]] <- list_all[[df]][-1,]
  }
  
  names(list_all[[df]]) <- gsub(" ", "_", names(list_all[[df]]) %>% tolower())
}

# names(list_all$Stay.At.Home) <- list_all$Stay.At.Home[1,]
# list_all$Stay.At.Home <- list_all$Stay.At.Home[-1,]


#Join the separate sheets into one master sheet
policy_df_0 <- list_all$State.Characteristics %>%
  select(state, state_abbreviation, state_fips_code,
         population = population_2018,
         area_sq_mi = square_miles,
         population_density = population_density_per_square_mile) %>%
  left_join(list_all$State.Of.Emergency %>%
              select(state, state_of_emergency_declared = state_of_emergency_issued,),
            by = "state") %>%
  left_join(list_all$Masks %>%
              select(state, begin_mask_mandate = public_face_mask_mandate, end_face_mask_mandate),
            by = "state") %>%
  mutate(begin_mask_mandate = as.Date(begin_mask_mandate, origin = "1899-12-30"),
         end_face_mask_mandate = as.Date(end_face_mask_mandate, origin = "1899-12-30")) %>%
  mutate(mask_mandate_flag = ifelse(year(begin_mask_mandate) > 2000, 1, 0)) %>%
  left_join(list_all$Stay.At.Home %>%
              select(state,
                     begin_stay_at_home = `stay_at_home/shelter_in_place`,
                     begin_stay_at_home2 = `stay-at-home_order_issued_but_did_not_specifically_restrict_movement_of_the_general_public`,
                     end_stay_at_home = `end_stay_at_home/shelter_in_place`),
            by = "state") %>%
  mutate(begin_stay_at_home = as.numeric(begin_stay_at_home),
         begin_stay_at_home2 = as.numeric(begin_stay_at_home2)) %>%
  mutate(begin_stay_at_home = case_when(
         begin_stay_at_home == 0 & begin_stay_at_home2 == 0 ~ 0,
         begin_stay_at_home == 0  ~ begin_stay_at_home2,
         begin_stay_at_home2 == 0  ~ begin_stay_at_home,
         TRUE ~ ifelse(begin_stay_at_home < begin_stay_at_home2, begin_stay_at_home, begin_stay_at_home2)
         ),
         begin_stay_at_home = as.Date(as.numeric(begin_stay_at_home), origin = "1899-12-30"),
         end_stay_at_home = as.Date(as.numeric(end_stay_at_home), origin = "1899-12-30")) %>%
  select(-begin_stay_at_home2) %>%
  mutate(stay_at_home_flag = ifelse(year(begin_stay_at_home) > 2000, 1, 0)) %>%
  left_join(list_all$Business.Closures %>%
              select(state, first_business_closures, first_business_reopening),
            by = "state") %>%
  left_join(list_all$Travel.Quarantines %>%
            select(state, quarantine_mandate_for_some_travelers,
                   quarantine_mandate_for_all_travelers,
                   end_travel_quarantine_mandate = quarantine_mandate_ended),
            by = "state") %>%
  mutate(quarantine_mandate_for_some_travelers = as.Date(quarantine_mandate_for_some_travelers, origin = "1899-12-30"),
         quarantine_mandate_for_all_travelers = as.Date(quarantine_mandate_for_all_travelers, origin = "1899-12-30")) %>%
  mutate(begin_travel_quarantine_mandate = ifelse(quarantine_mandate_for_some_travelers >
                                               quarantine_mandate_for_all_travelers,
                                               quarantine_mandate_for_some_travelers,
                                               quarantine_mandate_for_all_travelers),
         begin_travel_quarantine_mandate = as.Date(begin_travel_quarantine_mandate),
         end_travel_quarantine_mandate = as.Date(end_travel_quarantine_mandate, origin = "1899-12-30")) %>%
  mutate(travel_quarantine_mandate_flag = ifelse(year(begin_travel_quarantine_mandate) > 2000, 1, 0)) %>%
  select(-quarantine_mandate_for_some_travelers, -quarantine_mandate_for_all_travelers) %>%
  left_join(list_all$Unemployment.Benefits %>%
            select(state,
                   begin_increased_unemployment_benefits = extended_benefits_program_activated,
                   end_increased_unemployment_benefits = extended_benefits_program_deactivated,
                   increased_weekly_unemployment_insurance_amt_thru_jul31 =
                     "weekly_ui_maximum_amount_with_extra_stimulus_(through_july_31,_2020)_(dollars)"),
            by = "state") %>%
  mutate(begin_increased_unemployment_benefits =
           as.Date(as.numeric(begin_increased_unemployment_benefits),
                   origin = "1899-12-30"),
         end_increased_unemployment_benefits =
           as.Date(as.numeric(end_increased_unemployment_benefits),
                   origin = "1899-12-30"),
         increased_weekly_unemployment_insurance_amt_thru_jul31 =
           as.numeric(increased_weekly_unemployment_insurance_amt_thru_jul31))
  

saveRDS(policy_df_0, file = "pwd/AggregatedPolicyData.rds")
```

```{r load census, county population, mobility, and covid data, echo=FALSE, warning=FALSE, message=FALSE}
#Read in Census Data
acs_with_overlays <- read_csv("data/ACSDP1Y2019.DP05_data_with_overlays_2021-03-18T145421.csv", 
                              skip = 1, 
                              col_names = TRUE)

names(acs_with_overlays) <-  names(acs_with_overlays) %>% 
    tolower() %>%
    str_to_title() %>%
    gsub(pattern = " ", replacement = ".") %>%
    gsub(pattern = "!!", replacement = ".") %>%
    gsub(pattern = "!", replacement = ".")  


US_mobility_data_0 <- read_csv("data/2020_US_Region_Mobility_Report.csv", 
#                              skip = 1, 
                              col_names = TRUE)

names(US_mobility_data_0) <-  names(US_mobility_data_0) %>% 
    tolower() %>%
    str_to_title() %>%
    gsub(pattern = " ", replacement = ".") #%>%
    #gsub(pattern = "!!", replacement = ".") #%>%
    #gsub(pattern = "!", replacement = ".")  


# Get the area data by state
#https://www2.census.gov/library/publications/2001/compendia/ccdb00/tabB1.pdf
#https://www.census.gov/library/publications/2001/compendia/ccdb00.html
US_population_by_area_0 <- read.xlsx("data/LND01_census_area.xlsx", 
#                              skip = 1, 
                              colNames = TRUE) %>% 
  select(Areaname,	Census_fips_code = STCOU, SqMi = LND110190D)

US_mobility_data <- US_mobility_data_0 %>% 
  left_join(US_population_by_area_0, by = "Census_fips_code") %>% 
  select(-Areaname)

# US_mobility_data_1 %>% select(Census_fips_code) %>%  unique() %>% View()

# Get the population data by state
# https://www.census.gov/data/datasets/time-series/demo/popest/2010s-counties-total.html#par_textimage_70769902
US_population_by_county_0 <- read_csv("data/co-est2019-alldata.csv", 
#                              skip = 1, 
                              col_names = TRUE) %>% #glimpse()
  # select(COUNTY) %>% unique()
  select(STNAME, CTYNAME, POPESTIMATE2019)

names(US_population_by_county_0) <-  names(US_population_by_county_0) %>% 
    tolower() %>%
    str_to_title()

US_population_by_county <- US_population_by_county_0 %>% 
  group_by(Stname) %>% 
  mutate(StatePop = sum(Popestimate2019)/2) %>% #there is also a state level entry
  ungroup() %>% 
  mutate(CountyPopPerc = Popestimate2019/StatePop) %>% 
  arrange(Stname, desc(CountyPopPerc)) 
          
# US_population_by_county %>% 
#    filter(Stname == "New York") %>% View()
   #summarise(PercSum = sum(CountyPopPerc_Reallocated)) 

# Check to see if the county names match the mobility data county names
US_population_by_county$PresentFlag <- 0
US_population_by_county$Sub_region_2 <- NA

for (i in unique(US_mobility_data$Sub_region_2)){
  US_population_by_county <- US_population_by_county %>% 
    mutate(PresentFlag = if_else(Ctyname %like% i, 1, PresentFlag),
           Sub_region_2 = ifelse(Ctyname %like% i, i, Sub_region_2))
}
US_population_by_county$PresentFlag <- replace_na(US_population_by_county$PresentFlag, 0) 
#US_population_by_county %>% count(PresentFlag)

#Join the mobility data, population data, and area data
US_mobility_data2 <- US_mobility_data %>% 
  dplyr::rename(Stname = Sub_region_1) %>%
  left_join(US_population_by_county %>% 
              select(Stname, 
                     Sub_region_2, 
                     CountyPopPerc,
                     CountyPop = Popestimate2019, 
                     StatePop), 
            by = c("Stname","Sub_region_2")) %>% 
  dplyr::rename(Countyname = Sub_region_2) %>%
  filter(Stname != Countyname)

#Check this date has all 50 states
stopifnot(US_mobility_data2 %>% 
  filter(Date == "2020-03-30") %>% 
  select(Stname) %>% unique() %>% nrow() == 50)

#Add pop dens
US_mobility_data2_highpopdens0 <- US_mobility_data2 %>% 
  mutate(county_pop_dens = CountyPop/SqMi)

# US_mobility_data %>% 
#   filter(Date == "2020-03-30") %>%
#   arrange(desc(county_pop_dens)) %>% 
#   View()

#ggplot(US_mobility_data2_highpopdens0 %>% 
#         filter(Date == "2020-03-30"), 
#       aes(log10(county_pop_dens))) +
#  geom_histogram(bins = 100)

#User input
pop_dens_filter <- 2000

#ggplot(US_mobility_data2_highpopdens0 %>% 
#         filter(Date == "2020-03-30", county_pop_dens < pop_dens_filter), 
#       aes((county_pop_dens))) +
#  geom_histogram(bins = 100)

#Flag for areas greater than the pop_dens_filter people per sqmi cuttoff
US_mobility_data2_highpopdens <- US_mobility_data2_highpopdens0 %>% 
    mutate(high_popdens_county_flag = ifelse(county_pop_dens > pop_dens_filter, 1, 0))


US_mobility_data2_county_perc_reallocated <- US_mobility_data2_highpopdens %>% 
  filter(Date == "2020-03-30") %>%
  select(Stname, Countyname, CountyPopPerc, high_popdens_county_flag, 
         StatePop, CountyPop) %>% 
  group_by(Stname) %>% 
  mutate(TotalStatePopAccountedFor = sum(CountyPopPerc, na.rm = T),
         CountyPopPerc_Reallocated = CountyPopPerc/TotalStatePopAccountedFor, 
         high_popdens_flag = max(high_popdens_county_flag, na.rm = T)) %>% 
  ungroup()


#QC that all reallocated perc weights totals to 100 
stopifnot(US_mobility_data2_county_perc_reallocated %>% 
  group_by(Stname) %>%
  summarise(PercSum = sum(CountyPopPerc_Reallocated)) %>% # View()
  filter(PercSum < 1.01 & PercSum > .99) %>% 
  nrow() == (US_mobility_data2_highpopdens$Stname %>% 
               unique() %>% length())) # Check no high pop states were dropped 

US_mobility_data2_perc_pop_highdens <- US_mobility_data2_county_perc_reallocated %>% 
  filter(high_popdens_county_flag == 1) %>% 
  group_by(Stname) %>% 
  summarise(high_popdens_pop_perc = sum(CountyPop, na.rm = T)/StatePop) %>% 
  ungroup() %>% 
  distinct(.keep_all = T) %>% 
  filter(!is.na(high_popdens_pop_perc))

#Join new percents and high pop density percents
US_mobility_data3 <- US_mobility_data2_highpopdens %>% 
  left_join(US_mobility_data2_county_perc_reallocated, 
            by = c("Stname", "Countyname")) %>% 
  left_join(US_mobility_data2_perc_pop_highdens, 
            by = "Stname")
  
#reset names
US_mobility_data <- US_mobility_data3
rm(US_mobility_data2, US_mobility_data3, US_population_by_county_0, 
   US_mobility_data2_highpopdens, US_mobility_data2_highpopdens0,
   US_population_by_county, US_population_by_area_0, US_mobility_data_0, 
   US_mobility_data2_county_perc_reallocated)


## Pull data from NYT COVID Database and plot summary
NYT_Data <- fread("https://raw.githubusercontent.com/nytimes/covid-19-data/master/us-states.csv")
#summary(NYT_Data)

## Convert date strings to dates. 
NYT_Data[,date:=as.Date(date)]
#summary(NYT_Data)

## Calculate cases on a time interval
start.date <- as.Date("2021-01-01")
end.date <- as.Date("2021-02-28")

interval.cases <- NYT_Data %>%
  arrange(state, 
          date == start.date) %>%
  group_by(state) %>%
  mutate(cases_inc = cases - lag(cases),
         deaths_inc = deaths - lag(deaths)) %>%
  ungroup() %>% 
  filter(cases_inc >= 0)

```

```{r join mobility and covid, echo=FALSE, message=FALSE, warning=FALSE}
#join mobility data with covid database

US_mobility_data_agg <- US_mobility_data %>% 
  dplyr::rename("state" = "Stname",
         "date" = "Date") %>%
  group_by(state, date, high_popdens_pop_perc) %>% 
  mutate(Retail_and_recreation_percent_change_from_baseline_weighted = 
           Retail_and_recreation_percent_change_from_baseline * CountyPopPerc_Reallocated,
         Grocery_and_pharmacy_percent_change_from_baseline_weighted = 
           Grocery_and_pharmacy_percent_change_from_baseline * CountyPopPerc_Reallocated,
         Parks_percent_change_from_baseline_weighted = 
           Parks_percent_change_from_baseline * CountyPopPerc_Reallocated,
         Transit_stations_percent_change_from_baseline_weighted =
           Transit_stations_percent_change_from_baseline * CountyPopPerc_Reallocated,
         Workplaces_percent_change_from_baseline_weighted = 
           Workplaces_percent_change_from_baseline * CountyPopPerc_Reallocated,
         Residential_percent_change_from_baseline_weighted = 
           Residential_percent_change_from_baseline * CountyPopPerc_Reallocated) %>% 
  summarize(Retail_and_recreation_percent_change_from_baseline_weighted = 
              sum(Retail_and_recreation_percent_change_from_baseline_weighted, na.rm = T),
            Grocery_and_pharmacy_percent_change_from_baseline_weighted = 
              sum(Grocery_and_pharmacy_percent_change_from_baseline_weighted, na.rm = T),
            Parks_percent_change_from_baseline_weighted = 
              sum(Parks_percent_change_from_baseline_weighted, na.rm = T),
            Transit_stations_percent_change_from_baseline_weighted = 
              sum(Transit_stations_percent_change_from_baseline_weighted, na.rm = T), 
            Workplaces_percent_change_from_baseline_weighted = 
              sum(Workplaces_percent_change_from_baseline_weighted, na.rm = T),
            Residential_percent_change_from_baseline_weighted = 
              sum(Residential_percent_change_from_baseline_weighted, na.rm = T)) %>% 
  ungroup()

US_cases_and_mobility <- US_mobility_data_agg %>% 
  left_join(interval.cases, by = c("state", "date"))

#QC the join
stopifnot(US_cases_and_mobility %>% nrow()
  == US_mobility_data_agg %>% nrow())

#US_cases_and_mobility %>% 
#  filter(is.na(cases_inc),
#         !is.na(state)) %>% 
#  select(state, cases_inc) %>% nrow()
#1000

#US_cases_and_mobility %>% nrow()
#19800

#1000/19800*100
#[1] 5.1% missing inc cases data
```

```{r data wrangling and final df, echo=FALSE, message=FALSE, warning=FALSE}
#For each state, read in the first 365 days of transit change data, beginning at the date the state declared an emergency 
first_365 <- US_cases_and_mobility %>% 
  left_join(policy_df_0, by = c("state")) %>%
  filter(as.Date(date) >= as.Date(state_of_emergency_declared) & 
           as.Date(date) <= as.Date(state_of_emergency_declared) + 364)
  

#Calculate the median county-population-weigthed value of transit changes for each state in the first 365 days after emergency declaration
transit_chg <- aggregate(first_365$Transit_stations_percent_change_from_baseline_weighted, by=list(state=first_365$state), FUN=median)  %>%
  dplyr::rename(transit = x)

retail_chg <- aggregate(first_365$Retail_and_recreation_percent_change_from_baseline_weighted,by=list(state=first_365$state), FUN=median) %>%
  dplyr::rename(retail = x)

residential_chg <- aggregate(first_365$Residential_percent_change_from_baseline_weighted, by=list(state=first_365$state), FUN=median) %>%
  dplyr::rename(residential = x)

grocery_chg <- aggregate(first_365$Grocery_and_pharmacy_percent_change_from_baseline_weighted, by=list(state=first_365$state), FUN=median) %>%
  dplyr::rename(grocery = x)

parks_chg <- aggregate(first_365$Parks_percent_change_from_baseline_weighted, by=list(state=first_365$state), FUN=median)  %>%
  dplyr::rename(parks = x)

workplace_chg <- aggregate(first_365$Workplaces_percent_change_from_baseline_weighted, by=list(state=first_365$state), FUN=median)  %>%
  dplyr::rename(workplace = x)

z <- data.frame(transit_chg$transit, retail_chg$retail, grocery_chg$grocery, parks_chg$parks, workplace_chg$workplace, residential_chg$residential) %>%
  dplyr::rename(Transit = transit_chg.transit, Retail = retail_chg.retail, Grocery = grocery_chg.grocery,
                Parks = parks_chg.parks, Workplace = workplace_chg.workplace, Residential = residential_chg.residential)

#Visualize the correlation between mobility changes
M <- cor(z)
mat1 <- data.matrix(M)
print(M)
corrplot(mat1, method = "color", tl.col = 'black', is.corr=FALSE, title = 'Correlation Matrix: Median Mobility Changes by Category', addCoefasPercent = TRUE,  addCoef.col = "white")

#Very high correlations between the mobility changes, except for Parks. We will first explore whether parks mobility change demonstrates a clear relationship with COVID cases, and then explore whether transit mobility shows a linear relationship with cases.

#Calculate the median county-population-weighted value of transit changes for each state in the first 365 days after emergency declaration
mobility_chg <- aggregate(first_365$Transit_stations_percent_change_from_baseline_weighted, by=list(state=first_365$state), FUN=median) %>%
  dplyr::rename(median_transit_change = x)


#Calculate the cumulative cases at 365 days post emergency declaration by state  
cases_at_365 <- aggregate(first_365$cases, by=list(state=first_365$state), FUN=max) %>%
  dplyr::rename(cum_cases_at_365d = x)

#Grab the % of population living in high density variable
high_dens <- aggregate(first_365$high_popdens_pop_perc, by=list(state=first_365$state), FUN=mean) %>%
  dplyr::rename(high_popdens_pop_perc = x)

#Replace NaN with 0
high_dens$high_popdens_pop_perc[is.nan(high_dens$high_popdens_pop_perc)]<-0

#Create our final dataframe!
final_df <- cases_at_365 %>%
  left_join(policy_df_0, by = c("state")) %>%
  left_join(parks_chg, by = c("state")) %>%
  left_join(mobility_chg, by = c("state")) %>%
  left_join(high_dens, by = c("state")) %>%
  mutate(cases_per_100k_at_365d = 100000*cum_cases_at_365d/population) %>%
  left_join(acs_with_overlays, by = c("state" = "Geographic.Area.Name"))


#Relationship between median parks mobility change and cumulative case count per 100k at 365 days post statement of emergency
ggplot(data = final_df) +
  geom_point(aes(x = parks,
                 y = cases_per_100k_at_365d)) +
  geom_smooth(aes(x = parks,
                 y = cases_per_100k_at_365d),
            colour = 'red',
            method = 'lm', se = FALSE) +
  geom_smooth(aes(x = parks,
                  y = cases_per_100k_at_365d),
            colour = 'blue',
            method = 'loess', se = TRUE) +
  labs(title = 'State-Level One Year Median Parks Mobility Change vs.\nCases Per 100K People at One Year After SOE',
       x='Median Daily Relative % Parks Mobility Change',
       y='Cases Per 100K People One Year After SOE') +
  theme(axis.text=element_text(size=10),
        axis.title=element_text(size=12))



#Relationship between median transit change and cumulative case count per 100k at 365 days post statement of emergency
ggplot(data = final_df) +
  geom_point(aes(x = median_transit_change,
                 y = cases_per_100k_at_365d)) +
  geom_smooth(aes(x = median_transit_change,
                 y = cases_per_100k_at_365d),
            colour = 'red',
            method = 'lm', se = FALSE) +
  geom_smooth(aes(x = median_transit_change,
                  y = cases_per_100k_at_365d),
            colour = 'blue',
            method = 'loess', se = TRUE) +
  labs(title = 'State-Level One Year Median Transit Mobility Change vs.\nCases Per 100K People at One Year After SOE',
       x='Median Daily Relative % Transit Mobility Change',
       y='Cases Per 100K People One Year After SOE') +
  theme(axis.text=element_text(size=10),
        axis.title=element_text(size=12))



#Assess whether to use mean or median change in transit: We chose median because there is some skew in the data
h <- hist(first_365$Transit_stations_percent_change_from_baseline_weighted, 
     main = paste("Histogram of Relative Daily Changes in Transit Mobility for US States:
                  First Year"),  
     xlab='Relative % Change From Baseline Mobility', ylab='Frequency (Days)', col='gray') 

#h$density = h$density = h$counts/sum(h$counts)
#plot(h, freq = FALSE, col='gray', main = paste("Histogram of Relative Daily Changes in Transit Mobility for US States:
#                  First Year"), xlab='Relative % Change From Baseline Mobility', ylab='Frequency (Days)')


#Simple regression model on the relationship between change in parks mobility and COVID cases per 100K people. 
model_p <- lm(cases_per_100k_at_365d ~ parks, data = final_df)

#Pull out the p-values for the simple regression coefficients
#summary(model_p)$coefficients[,4]
```

```{r data wrangling part 2: additional policy data wrangling, echo=FALSE, message=FALSE}
# Capture # of Days After State of Emergency that a particular policy was in place--------------------

# days of mask mandate --------------------
m1 <- select(final_df, begin_mask_mandate, end_face_mask_mandate, state_of_emergency_declared, cases_per_100k_at_365d) %>%
  mutate(state_of_emergency_declared = as.Date(state_of_emergency_declared))
# str(m1)
m1 <- m1 %>%
  mutate(diff_in_days = case_when(
      begin_mask_mandate == "1899-12-30" ~ 0,
      end_face_mask_mandate == "1899-12-30" &
      begin_mask_mandate != "1899-12-30" ~
        as.numeric(state_of_emergency_declared) -
      as.numeric(begin_mask_mandate) + 364,
    TRUE ~ as.numeric(end_face_mask_mandate) -
      as.numeric(begin_mask_mandate)
  ))

m1$diff_in_days[m1$diff_in_days > 365] <- 365
final_df$mask_mandate_days = m1$diff_in_days


m1 <- select(final_df, begin_increased_unemployment_benefits, end_increased_unemployment_benefits, state_of_emergency_declared, cases_per_100k_at_365d) %>%
  mutate(state_of_emergency_declared = as.Date(state_of_emergency_declared))
# str(m1)
m1 <- m1 %>%
  mutate(diff_in_days = case_when(
      begin_increased_unemployment_benefits == "1899-12-30" ~ 0,
    end_increased_unemployment_benefits == "1899-12-30" &
      begin_increased_unemployment_benefits != "1899-12-30" ~
        as.numeric(state_of_emergency_declared) -
      as.numeric(begin_increased_unemployment_benefits) + 364,
    TRUE ~ as.numeric(end_increased_unemployment_benefits) -
      as.numeric(begin_increased_unemployment_benefits)
  ))

m1$diff_in_days[m1$diff_in_days > 365] <- 365
final_df$unemployment_benefits_days = m1$diff_in_days



# days of stay at home mandate  --------------------
m1 <- select(final_df, begin_stay_at_home, end_stay_at_home, state_of_emergency_declared, cases_per_100k_at_365d) %>%
  mutate(state_of_emergency_declared = as.Date(state_of_emergency_declared))
# str(m1)
m1 <- m1 %>%
  mutate(diff_in_days = case_when(
      begin_stay_at_home == "1899-12-30" ~ 0,
      end_stay_at_home == "1899-12-30" &
      begin_stay_at_home != "1899-12-30" ~
        as.numeric(state_of_emergency_declared) -
      as.numeric(begin_stay_at_home) + 364,
    TRUE ~ as.numeric(end_stay_at_home) -
      as.numeric(begin_stay_at_home)
  ))

m1$diff_in_days[m1$diff_in_days > 365] <- 365
final_df$stay_at_home_days = m1$diff_in_days



# days of travel quarantine mandate  --------------------
m1 <- select(final_df, begin_travel_quarantine_mandate, end_travel_quarantine_mandate, state_of_emergency_declared, cases_per_100k_at_365d) %>%
  mutate(state_of_emergency_declared = as.Date(state_of_emergency_declared))
# str(m1)
m1 <- m1 %>%
  mutate(diff_in_days = case_when(
      begin_travel_quarantine_mandate == "1899-12-30" ~ 0,
      end_travel_quarantine_mandate == "1899-12-30" &
      begin_travel_quarantine_mandate != "1899-12-30" ~
        as.numeric(state_of_emergency_declared) -
      as.numeric(begin_travel_quarantine_mandate) + 364,
    TRUE ~ as.numeric(end_travel_quarantine_mandate) -
      as.numeric(begin_travel_quarantine_mandate)
  ))

m1$diff_in_days[m1$diff_in_days > 365] <- 365
final_df$travel_quarantine_mandate_days = m1$diff_in_days



# days of business closures (first wave)  --------------------

m1 <- select(final_df, first_business_closures, first_business_reopening, state_of_emergency_declared, cases_per_100k_at_365d) %>%
  mutate(state_of_emergency_declared = as.Date(state_of_emergency_declared),
         first_business_closures = as.Date(first_business_closures),
         first_business_reopening = as.Date(first_business_reopening)
         )
# str(m1)
m1 <- m1 %>%
  mutate(diff_in_days = case_when(
      first_business_closures == "1899-12-30" ~ 0,
      first_business_reopening == "1899-12-30" &
      first_business_closures != "1899-12-30" ~
        as.numeric(state_of_emergency_declared) -
      as.numeric(first_business_closures) + 364,
    TRUE ~ as.numeric(first_business_reopening) -
      as.numeric(first_business_closures)
  ))

# m1$diff_in_days[m1$diff_in_days > 365] <- 365
final_df$business_closed_days_round1 = m1$diff_in_days
```

After inspection of the correlation matrix between the different measures of mobility changes, we chose to use the state-level median change in transit mobility in the 365 days after each state declared an emergency as our main variable of interest for Model 1. Our motivation for choosing transit over the other mobility features was as follows:

1) Transit is most closely aligned with our understanding of how viruses spread, particularly from one locality or population center to another. 

2) The mobility changes in Transit, Retail, Grocery, and Workplace are highly positively correlated with each other (>0.8 in each pair), and highly negatively correlated with Residential mobility changes (absolute value of >0.7 or above). Highly correlated features should be avoided in descriptive and explanatory linear regression modeling as they tend to increase the standard error estimates on the model parameter estimates for the correlated features. 

The only mobility metric that was not strongly correlated with the others was Parks. Since the median change in Park mobility was not strongly correlated with any other mobility changes, we examined its relationship with our target variable but did not observe a positive or negative linear relationship. A t-Test on our calculated simple regression coefficient failed to reject the null hypothesis that there was no evidence that the coefficient for change in median Parks mobility was measurably different than zero. 

3) We did not want to take an aggregation of the set of highly correlated features (Transit, Retail, Grocery, and Workplace) because the raw data provided are relative numbers and we do not have access to the underlying absolute mobility data, so we would be unable to derive a correct weighted average of these features.

4) Given that the distribution of relative transit mobility change had a left skew, our team decided to use the median value for each state within the 365 day window as a better measure of central tenancy. 

# Base Model

```{r model 1, echo=FALSE, message=FALSE}
model_1_final <- lm(cases_per_100k_at_365d ~ median_transit_change, data = final_df)

cat("\n-------Model results-------\n")
summary(model_1_final)
cat("\n")

#-------Model Residual Plot-------
qplot(model_1_final$residuals,
               geom = "histogram", bins = 20) +
         labs(title = "Histogram of Model 1 Residuals",
              x = "residual")

qplot(model_1_final$fitted, model_1_final$residuals,
            geom = "point") +
         geom_abline(intercept = 0,
                     slope = 0,
                     colour = "red") + 
         geom_smooth(aes(x = model_1_final$fitted,
                         y = model_1_final$residuals),
            colour = 'blue',
            method = "loess", se = TRUE) +
         labs(title = "Plot of residuals vs fitted values",
              x = "fitted value",
              y = "residual")


cat("\n-------Homoskedasticity Test-------\n")
lmtest::bptest(model_1_final)
cat("\n")


cat("\n------Normality of Residuals Test-------\n")
shapiro.test(model_1_final$residuals)
cat("\n")
```

# Model 1 Discussion Here:

# Second Model. 

As we thought about an approach to building our Model 2 specifications, the general approach that our group decided to take was to incrementally test the addition of new features to our Model 1 specification in descending order of expected importance, according to our general understanding of how viruses spread. At each iteration, we began by examining the relationship (using scatterplots) between a new demographic or policy-related feature and our outcome of interest: COVID cases per 100K people 365 days after each state declared a state of emergency. If we observed a relationship, we ran a combination of t-Tests and/or ANOVA F-Tests to determine whether or not to add the feature to our Model 1 specification. We proceeded in this 'greedy' algorithmic fashion by adding variables to our Model 1 specification until we could no longer justify further additions based on results from ANOVA F-Tests. The first incremental feature we tested was derived from observational data from the census bureau on population age distributions by state. 

One of the contributing factors to the spread of COVID-19 is asymptomatic spread. Younger people have been shown to have milder symptoms and therefore it stands to reason that they may be less likely to get tested, and ultimately end up spreading the disease at a greater rate than older age groups. To make matters worse, younger people tend to interact with more people (source?) as a result of being in school (switching between classrooms, touching desks and other surfaces where other students have been, congregating in large cafeterias, etc.), which increases the probability of viral transmission. Hence, for our second model, we were interested in testing our general hypothesis that age demographics may play a role in the spread of COVID-19. We began by doing some basic exploratory data analysis (EDA) with respect to age distributions and cumulative COVID-19 case counts per 100K, which is our target variable for Lab 2.

```{r age EDA < 24, echo=FALSE, message=FALSE}
census_df<-final_df[c('cases_per_100k_at_365d', 'Percent.Sex.And.Age.Total.Population.Under.5.Years', 'Percent.Sex.And.Age.Total.Population.5.To.9.Years', 'Percent.Sex.And.Age.Total.Population.10.To.14.Years', 'Percent.Sex.And.Age.Total.Population.15.To.19.Years','Percent.Sex.And.Age.Total.Population.20.To.24.Years')]

names(census_df) <- names(census_df) %>% gsub(pattern = "Percent.Sex.And.Age.Total.Population.", replacement = "Pct. ") %>%  gsub(pattern = ".To.", replacement = "- \n") %>%  gsub(pattern = ".Years", replacement = " Yrs") %>%  gsub(pattern = 'cases_per_100k_at_365d', replacement = " Cases per \n 100k")

panel.hist <- function(x, ...)
{
    usr <- par("usr"); on.exit(par(usr))
    par(usr = c(usr[1:2], 0, 1.5) )
    h <- hist(x, plot = FALSE)
    breaks <- h$breaks; nB <- length(breaks)
    y <- h$counts; y <- y/max(y)
    rect(breaks[-nB], 0, breaks[-1], y, col = "cyan", ...)
}

panel.cor<-function(x,y)
  {
    usr<-par("usr"); on.exit(par(usr))
    par(usr=c(0,1,0,1))
    r=round(cor(x,y),digits=2)
    text(0.5,0.5,r)
  }

pairs(census_df,lower.panel = panel.cor,upper.panel = panel.smooth, diag.panel = panel.hist, cex.labels=1)
```


```{r age EDA 25-59, echo=FALSE, message=FALSE}
census_df<-final_df[c('cases_per_100k_at_365d', 'Percent.Sex.And.Age.Total.Population.25.To.34.Years','Percent.Sex.And.Age.Total.Population.35.To.44.Years','Percent.Sex.And.Age.Total.Population.45.To.54.Years', 'Percent.Sex.And.Age.Total.Population.55.To.59.Years')]

names(census_df) <- names(census_df) %>% gsub(pattern = "Percent.Sex.And.Age.Total.Population.", replacement = "Pct. ") %>%  gsub(pattern = ".To.", replacement = "- \n") %>%  gsub(pattern = ".Years", replacement = " Yrs") %>%  gsub(pattern = 'cases_per_100k_at_365d', replacement = " Cases per \n 100k")

panel.cor<-function(x,y)
  {
    usr<-par("usr"); on.exit(par(usr))
    par(usr=c(0,1,0,1))
    r=round(cor(x,y),digits=2)
    text(0.5,0.5,r)
  }

pairs(census_df,lower.panel = panel.cor,upper.panel = panel.smooth, diag.panel = panel.hist, cex.labels=1)
```

```{r age EDA 60+, echo=FALSE, message=FALSE}
census_df<-final_df[c('cases_per_100k_at_365d','Percent.Sex.And.Age.Total.Population.60.To.64.Years','Percent.Sex.And.Age.Total.Population.65.To.74.Years', 'Percent.Sex.And.Age.Total.Population.75.To.84.Years', 'Percent.Sex.And.Age.Total.Population.85.Years.And.Over')]

names(census_df) <- names(census_df) %>% gsub(pattern = "Percent.Sex.And.Age.Total.Population.", replacement = "Pct. ") %>%  gsub(pattern = ".To.", replacement = "- \n") %>%  gsub(pattern = ".Years", replacement = " Yrs") %>%  gsub(pattern = 'cases_per_100k_at_365d', replacement = " Cases per \n 100k")

panel.cor<-function(x,y)
  {
    usr<-par("usr"); on.exit(par(usr))
    par(usr=c(0,1,0,1))
    r=round(cor(x,y),digits=2)
    text(0.5,0.5,r)
  }

pairs(census_df,lower.panel = panel.cor,upper.panel = panel.smooth, diag.panel = panel.hist, cex.labels=1)
```


```{r age demo F-tests for feature creation, echo=FALSE, message=FALSE}

#Build nested models for categorical age demographic data where visual inspection suggested interesting relationships with target
model_a = lm(cases_per_100k_at_365d ~ median_transit_change + Percent.Sex.And.Age.Total.Population.Under.5.Years, data = final_df)

model_b = lm(cases_per_100k_at_365d ~ median_transit_change + Percent.Sex.And.Age.Total.Population.Under.5.Years + Percent.Sex.And.Age.Total.Population.5.To.9.Years, data = final_df)

model_c = lm(cases_per_100k_at_365d ~ median_transit_change + Percent.Sex.And.Age.Total.Population.Under.5.Years + Percent.Sex.And.Age.Total.Population.5.To.9.Years + Percent.Sex.And.Age.Total.Population.10.To.14.Years, data = final_df)

model_d = lm(cases_per_100k_at_365d ~ median_transit_change + Percent.Sex.And.Age.Total.Population.Under.5.Years + Percent.Sex.And.Age.Total.Population.5.To.9.Years + Percent.Sex.And.Age.Total.Population.10.To.14.Years + Percent.Sex.And.Age.Total.Population.15.To.19.Years, data = final_df)

model_e = lm(cases_per_100k_at_365d ~ median_transit_change + Percent.Sex.And.Age.Total.Population.Under.5.Years + Percent.Sex.And.Age.Total.Population.5.To.9.Years + Percent.Sex.And.Age.Total.Population.10.To.14.Years + Percent.Sex.And.Age.Total.Population.15.To.19.Years + Percent.Sex.And.Age.Total.Population.20.To.24.Years, data = final_df)


#Significant 
anova(model_a, model_1_final, test = "F")

#Insignificant 
anova(model_b, model_a, test = "F")

#Significant F-test
anova(model_c, model_a, test = "F")

#Insignificant relative to model_c
anova(model_d, model_c, test = "F")
anova(model_e, model_d, test = "F")
anova(model_e, model_c, test = "F")
```

When performing EDA on the categorical age group distributions and their relationship to our target variable, we noticed a strong positive correlation to the target among age groups 0-24. Age groups in the 24+ range did not appear to follow a consistent pattern with respect to correlation with our target variable and were therefore not a key focus area for our analysis.

Next, we performed a series of ANOVA F-tests on a nested set of linear regression models with parameter estimates for this set of age groups (Under 5, 5-9, 10-14, 15-19, and 20-24) as well as our main variable of interest (median transit mobility change). Our motivation here was to understand whether the regression residuals were measurably different from one another between the different (nested) model specifications. For context, the null hypothesis for an F-test is that fitting additional coefficients for a longer model does not measurably reduce the residuals relative to a nested model with fewer parameters. 

Our first ANOVA F-test compared our Model 1 with a new model that had an additional parameter estimate for the percentage of the population under 5 years old. With a p-value of 0.0003, we rejected the null hypothesis and used this new model (model_a) as the baseline model for subsequent comparisons with additional parameter estimates for different age categories. 

We failed to reject the null hypothesis when adding an estimator for ages 5-9 (model_b), and succeeded in rejecting the null hypothesis when adding an estimator for ages 10-14 (model_c), with a p-value of 0.001. Adding additional estimators for 15-19 and 20-24 failed to reject the null hypothesis that these models (model_d and model_e) were measurably better at reducing residuals than model_c. These results suggested creating two new features for the percentage of the population ages 0-9 and 10-24 and adding them to our base Model 1 to create the first specification for our Model 2.

```{r df update 1, echo=FALSE, message=FALSE} 
final_df <- final_df %>% mutate(pop_pct_age_0_9 = Percent.Sex.And.Age.Total.Population.Under.5.Years + Percent.Sex.And.Age.Total.Population.5.To.9.Years,
                    pop_pct_age_10_24 = Percent.Sex.And.Age.Total.Population.10.To.14.Years + Percent.Sex.And.Age.Total.Population.15.To.19.Years + Percent.Sex.And.Age.Total.Population.20.To.24.Years)

cor(final_df$pop_pct_age_0_9, final_df$pop_pct_age_10_24)

final_df <- final_df %>% mutate(pop_pct_age_0_24 = Percent.Sex.And.Age.Total.Population.Under.5.Years + Percent.Sex.And.Age.Total.Population.5.To.9.Years + Percent.Sex.And.Age.Total.Population.10.To.14.Years + Percent.Sex.And.Age.Total.Population.15.To.19.Years + Percent.Sex.And.Age.Total.Population.20.To.24.Years)
```

However, after measuring a 0.77 correlation between these two features - and with the goal of increased model parsimony - we decided to group them together to prevent the standard errors for their respective coefficients from increasing substantially. Let's take a look and assess whether this new variable for percentage of the population < 24 years old visually satisfies the conditional linearity expectation with respect to our target.

```{r incremental variables 1, echo=FALSE, message=FALSE}
ggplot(data = final_df) +
  geom_point(aes(x = pop_pct_age_0_24,
                 y = cases_per_100k_at_365d)) +
  geom_smooth(aes(x = pop_pct_age_0_24,
                 y = cases_per_100k_at_365d),
            colour = 'red',
            method = 'lm', se = FALSE) +
  geom_smooth(aes(x = pop_pct_age_0_24,
                  y = cases_per_100k_at_365d),
            colour = 'blue',
            method = 'loess', se = TRUE) +
  labs(title = 'Percentage of State Population Under Age 25 vs. \nCases Per 100K People at One Year After SOE',
       x='Percentage of State Population Under Age 25',
       y='Cases Per 100K People One Year After SOE') +
  theme(axis.text=element_text(size=10),
        axis.title=element_text(size=12))
```

The conditional linear expectation between our population percentage aged 0-24 and our target appears to be met. In addition to evaluating the linear relationships between the demographic age variables and our target, we also evaluated the log, square root, and square transformations to the age distribution data but did not find them to aid in reducing the frequency or magnitude of outlier data points. 

Indeed, including a feature for the percentage of the population aged < 24 years old appears to have improved our model's performance. An ANOVA F-test returned a p-value of 0.0001, suggesting that we reject the null hypothesis that the (interim) Model 2's residuals were not measurably different from the residuals of Model 1.

```{r model 2 - part 1, echo=FALSE, message=FALSE}
model_2a <- lm(cases_per_100k_at_365d ~ median_transit_change + pop_pct_age_0_24, data = final_df)
# Significant
anova(model_2a, model_1_final, test = "F")
```

After accounting for changes in mobility and demographic age differences between states, the next variable we wanted to explore as part of our descriptive model for COVID case counts was population density. According to the World Health Organization, "COVID-19 virus is primarily transmitted between people through respiratory droplets and contact routes." (https://www.who.int/news-room/commentaries/detail/modes-of-transmission-of-virus-causing-covid-19-implications-for-ipc-precaution-recommendations). In other words, COVID spreads primarily through physical interactions between infected and uninfected hosts, regardless of whether the actual mechanism of transmission is airborne or surface based contact. Hence, it stands to reason that more densely populated areas would see greater rates of infections, because the frequency of these physical interactions will increase with population density. This was the motivation for our group exploring whether a relationship existed between state population density and our outcome variable of interest.


```{r incremental variables 2, echo=FALSE, message=FALSE}
ggplot(data = final_df) +
  geom_point(aes(x = population_density,
                 y = cases_per_100k_at_365d)) +
  geom_smooth(aes(x = population_density,
                 y = cases_per_100k_at_365d),
            colour = 'red',
            method = 'lm', se = FALSE) +
  geom_smooth(aes(x = population_density,
                  y = cases_per_100k_at_365d),
            colour = 'blue',
            method = 'loess', se = TRUE) +
  labs(title = 'Population Density vs. \nCases Per 100K People at One Year After SOE',
       x='Population Density (people/sq. mi)',
       y='Cases Per 100K People One Year After SOE') +
  theme(axis.text=element_text(size=10),
        axis.title=element_text(size=12))
```

To our surprise, however, there was no clear relationship between population density and our outcome variable. A t-Test on the parameter estimate for population density's relationship with our target variable failed to reject the null hypothesis that the coefficient value was not measurably different from zero. 

```{r simple population density model, echo=FALSE, message=FALSE}
model_density <- lm(cases_per_100k_at_365d ~ population_density , data = final_df)
summary(model_density)
```

In spite of this test, our group decided to move forward and try including the population density feature in Model 2, as we believed it to be a conceptually meaningful variable in describing the population prevalence of COVID in each state one year after each state declared a state of emergency. Therefore, we proceeded with an ANOVA F-Test to test whether the incremental population density feature measurably improved model performance (via reduction of residuals) relative to our current model with features for median transit mobility change and percentage of the population < 24 years old. This ANOVA F-test returned a p-value of 0.002, enough to reject the null hypothesis that the model residuals were not measurably different from one another.


```{r model 2 - part 2, echo=FALSE, message=FALSE}
model_2_final <- lm(cases_per_100k_at_365d ~ median_transit_change + pop_pct_age_0_24 + population_density, data = final_df)

cat("\n-------Model results-------\n")
summary(model_2_final)
cat("\n")

# Significance Test Relative To Model 1
cat("\n-------ANOVA F-Test Significance Test Relative To Model 1-------\n")
anova(model_2_final, model_1_final, test = "F")
cat("\n")

# Significance Test Relative To Interim Model 2
cat("\n-------ANOVA F-Test Significance Test Relative To Interim Model 2-------\n")
anova(model_2_final, model_2a, test = "F")
cat("\n")

#-------Model Residual Plot-------
qplot(model_2_final$residuals,
               geom = "histogram", bins = 20) +
         labs(title = "Histogram of Model 2 Residuals",
              x = "residual")

qplot(model_2_final$fitted, model_2_final$residuals,
            geom = "point") +
         geom_abline(intercept = 0,
                     slope = 0,
                     colour = "red") + 
         geom_smooth(aes(x = model_2_final$fitted,
                         y = model_2_final$residuals),
            colour = 'blue',
            method = "loess", se = TRUE) +
         labs(title = "Plot of Residuals vs Fitted Values",
              x = "fitted value",
              y = "residual")


cat("\n-------Homoskedasticity Test-------\n")
lmtest::bptest(model_2_final)
cat("\n")


cat("\n------Normality of Residuals Test-------\n")
shapiro.test(model_2_final$residuals)
cat("\n")
```

Because the population density feature only becomes statistically significant at the p = 0.05 level in our model when we include features for median change in transit mobility and the percentage of the population under age 25, we say that the population density has a conditional relationship with our outcome of interest. Both of these two co-variates (median transit mobility change and percentage of the population under age 25) are negatively correlated with population density and positively correlated with our outcome variable. 

```{r conditional effect population density, echo=FALSE, message=FALSE}
ggplot(data = final_df) +
  geom_point(aes(y = population_density,
                 x = median_transit_change)) +
  geom_smooth(aes(y = population_density,
                 x = median_transit_change),
            colour = 'red',
            method = 'lm', se = FALSE) +
  geom_smooth(aes(y = population_density,
                  x = median_transit_change),
            colour = 'blue',
            method = 'loess', se = TRUE) +
  labs(title = 'Population Density vs. \nMedian Transit Mobility Change',
       y='State Population Density (people/sq. mi)',
       x='State-Level One Year Median Transit Mobility Change') +
  theme(axis.text=element_text(size=10),
        axis.title=element_text(size=12))


ggplot(data = final_df) +
  geom_point(aes(y = population_density,
                 x = pop_pct_age_0_24)) +
  geom_smooth(aes(y = population_density,
                 x = pop_pct_age_0_24),
            colour = 'red',
            method = 'lm', se = FALSE) +
  geom_smooth(aes(y = population_density,
                  x = pop_pct_age_0_24),
            colour = 'blue',
            method = 'loess', se = TRUE) +
  labs(title = 'Population Density vs. \nPercent of Population Under Age 25',
       y='State Population Density (people/sq. mi)',
       x='Percentage of the State Population Under Age 25') +
  theme(axis.text=element_text(size=10),
        axis.title=element_text(size=12))

```

However, we are only interested in the unique variation of population density with respect to our outcome variable. When the OLS regression algorithm calculates the parameter estimate for population density, it starts by regressing population density on the other model co-variate (input) features. The residuals from that regression represent the portion of population density that is *not* colinear with median transit mobility change and percentage of the population under age 25. Then OLS regresses our target values on those residuals to derive an estimate for the population density parameter. The model summary tells us that if we hold the percentage of the population under 25 and the median transit mobility change for a state constant (we do not allow them to co-vary), that there exists a positive correlation between population density and our outcome variable at a statistically significant level (p = 0.002) using classical standard errors.

At this stage, we wanted to explore whether any state policy changes aimed at reducing the spread of COVID-19 added incremental descriptive power beyond our current Model 2 specification. In particular, we wanted to examine whether the timing of mask mandates, length and amount of increased government assistance via enhanced unemployment benefits, business closures, stay at home mandates, and travel quarantine restrictions had measurable effects on our outcome of interest after accounting for the features already in our Model 2 specification (which included state-level features for median change in transit mobility, percentage of the population under 25 years of age, and population density). All the policy-related features were recorded as dates, except for the increased unemployment insurance amount, which was recorded as an integer. 

To align with our target variable of COVID-19 Cases per 100K one year after state of emergency declaration, we encoded the date related policy variables as the total number of days each policy was in place for after the state of emergency was announced for each state (up to 365 days). For transparency:

1. If a policy had no beginning and end dates, the total days were assigned as zero.
2. If a policy had beginning but not end dates, the total days were calculated by the date of state emergency declared + 364 days - the date of the beginning of the policy.
3. If a policy had both beginning and end dates, the total days were calculated by the difference in days of the two dates.

Once the policy-related features of our interest were transformed into days in force, we conducted EDA on the policy-features and COVID cases per 100k population using scatter plot and correlation matrices. 

```{r policy days vs cases per capita, echo=FALSE, message=FALSE}

# Plot first half of policy vars
m1 <- select(final_df, cases_per_100k_at_365d, 
             mask_mandate_days,
             unemployment_benefits_days,
             increased_weekly_unemployment_insurance_amt_thru_jul31
            )

panel.cor<-function(x,y)
  {
    usr<-par("usr"); on.exit(par(usr))
    par(usr=c(0,1,0,1))
    r=round(cor(x,y),digits=2)
    text(0.5,0.5,r)
  }

pairs(m1,lower.panel = panel.cor,upper.panel = panel.smooth, diag.panel = panel.hist, cex.labels=1)



# Plot second half of policy vars
m2 <- select(final_df, cases_per_100k_at_365d, 
             business_closed_days_round1,
             travel_quarantine_mandate_days,
             stay_at_home_days
            )

panel.cor<-function(x,y)
  {
    usr<-par("usr"); on.exit(par(usr))
    par(usr=c(0,1,0,1))
    r=round(cor(x,y),digits=2)
    text(0.5,0.5,r)
  }

pairs(m2,lower.panel = panel.cor,upper.panel = panel.smooth, diag.panel = panel.hist, cex.labels=1)

```
```{r policy correlation plot, echo=FALSE, message=FALSE}
z <- data.frame(final_df$cases_per_100k_at_365d, 
             final_df$mask_mandate_days,
             final_df$unemployment_benefits_days,
             final_df$increased_weekly_unemployment_insurance_amt_thru_jul31,
             final_df$business_closed_days_round1,
             final_df$travel_quarantine_mandate_days,
             final_df$stay_at_home_days) %>%
  dplyr::rename(cases_per_100k = final_df.cases_per_100k_at_365d, 
             mask_days = final_df.mask_mandate_days,
             ue_ben_days = final_df.unemployment_benefits_days,
             ue_ins_amt = final_df.increased_weekly_unemployment_insurance_amt_thru_jul31,
             biz_closed_days = final_df.business_closed_days_round1,
             quarantine_days = final_df.travel_quarantine_mandate_days,
             stay_at_home_days = final_df.stay_at_home_days)

#Visualize the correlation between state policies and cases
M <- cor(z)
mat1 <- data.matrix(M)
print(M)
corrplot(mat1, method = "color", tl.col = 'black', is.corr=FALSE, title = 'Correlation Matrix: State Policies vs. Cases', addCoefasPercent = TRUE,  addCoef.col = "white")
```

From the scatterplots, we see varying degrees of linearity between the policy features and our outcome of interest. We tested each of these features iteratively in the same manner as before, using significance from ANOVA F-tests as the benchmark to decide whether or not to include incremental policy features as part of our final Model 2. Ultimately, none of these policy variables (length of mask mandates, length and amount of increased government assistance via enhanced unemployment benefits, length business closures, length of stay at home mandates, and length of travel quarantine restrictions) returned a p-value that would allow us to reject the null hypothesis that the model residuals had not measurably improved.

Still, it is worth noting that all of the aforementioned policy features, which were declared to mitigate COVID spread, demonstrated a negative correlation (from approximately -0.5 to -0.2) with our outcome variable of COVID cases per 100k people. The fact that these policy features failed to reject the null hypothesis in the ANOVA F-Test relative to the features already in our Model 2 was not entirely unexpected. Conceptually, several of the features share a significant amount information with median transit mobility change. One could make the argument that business closures, quarantine mandates, and stay at home mandates are all captured, to some extent, in the transit mobility change. 

# Ryan fleshing out writing here

Unemployment insurance extensions and benefit increases

We were surprised, however, that mask mandates 


# Jun's policy tests (all failed) for Model 2 here (to be reviewed...):

```{r base model, echo=FALSE, message=FALSE, results=FALSE}

#model_base=cases_per_100k_at_365d ~ median_transit_change + pop_pct_age_0_24 + population_density 
#summary(lm.I)
#resid(lm.I)
#coeftest(m, vcov. = vcovHC(m, type = 'HC1'))
#shapiro.test(lm.II$residuals)
#bptest(lm.II)
```


```{r pack the plot, echo=FALSE, message=FALSE, results=FALSE}

resid<-function(lm.model)
  {
    qplot(lm.model$residuals,
          geom = "histogram", bins = 20) +
          labs(title = "Histogram of residuals",x = "residual")

    qplot(lm.model$fitted, lm.model$residuals,
            geom = "point") +
          geom_abline(intercept = 0,
                     slope = 0,
                     colour = "red") +
          geom_smooth(aes(x = lm.model$fitted,
                         y = lm.model$residuals),
                         colour = 'blue',
                        method = "loess", se = TRUE) +
         labs(title = "Plot of residuals vs fitted values", x = "fitted value", y = "residual")
}



```

```{r model2 adding mask_mandate_days to base model}

#model_base=cases_per_100k_at_365d ~ median_transit_change + pop_pct_age_0_24 + population_density 
#Model_3=cases_per_100k_at_365d ~ median_transit_change + pop_pct_age_0_24 + population_density + mask_mandate_days+unemployment_benefits_days #+increased_weekly_unemployment_insurance_amt_thru_jul31+business_close_open_days+travel_quarantine_mandate_days+stay_at_home_days

#lm.II=lm(Model_3,data=final_df)
#summary(lm.II)
#resid(lm.II)
#anova(lm.II, lm.I, test = "F")
#shapiro.test(lm.II$residuals)
#bptest(lm.II)
```

<!-- ```{r check models treat policy as indicator variable using a flag} -->

<!-- model_I=cases_per_100k_at_365d ~ median_transit_change + pop_pct_age_0_24 + population_density  -->
<!-- model_II_flag=cases_per_100k_at_365d ~ median_transit_change + pop_pct_age_0_24 + population_density +travel_quarantine_mandate_flag -->
<!-- model_III_flag=cases_per_100k_at_365d ~ median_transit_change + pop_pct_age_0_24 + population_density +stay_at_home_flag -->
<!-- model_IV_flag=cases_per_100k_at_365d ~ median_transit_change + pop_pct_age_0_24 + population_density +mask_mandate_flag -->
<!-- ``` -->

<!-- ```{r adding travel_quarantine_mandate_flag as an indicator variable to base model} -->
<!-- model_II_flag -->
<!-- lm.II_flag=lm(model_II_flag,data=final_df) -->
<!-- summary(lm.II_flag) -->
<!-- resid(lm.II_flag) -->
<!-- shapiro.test(lm.II_flag$residuals) -->
<!-- bptest(lm.II_flag) -->
<!-- coeftest(lm.II_flag, vcov = vcovHC(lm.II_flag, type = "HC0")) -->
<!-- ``` -->

<!-- ```{r adding stay_at_home_flag as an indicator variable to base mode} -->
<!-- model_III_flag -->
<!-- lm.III_flag=lm(model_III_flag,data=final_df) -->
<!-- summary(lm.III_flag) -->
<!-- resid(lm.III_flag) -->
<!-- shapiro.test(lm.III_flag$residuals) -->
<!-- bptest(lm.III_flag) -->
<!-- coeftest(lm.III_flag, vcov = vcovHC(lm.III_flag, type = "HC0")) -->
<!-- ``` -->

<!-- ```{r adding mask_mandate_flag  as an indicator variable to base mode} -->
<!-- model_IV_flag -->
<!-- lm.IV_flag=lm(model_IV_flag,data=final_df) -->
<!-- summary(lm.IV_flag) -->
<!-- resid(lm.IV_flag) -->
<!-- shapiro.test(lm.IV_flag$residuals) -->
<!-- bptest(lm.IV_flag) -->
<!-- ``` -->


# Third Model

$$ 
\text{ Model  3  }:\   
\begin{equation}
  \text{cases_per_100k_at_365d}=\beta_0 + \beta_1 \text{median_transit_change} + \beta_2 \text{pop_pct_age_0_24} + \beta_3 \text{population_density} + \beta_4    \text{mask_mandate_days} + \beta_5    \text{unemployment_benefits_days}+\beta_6    \text{increased_weekly_unemployment_insurance_amt_thru_jul3}+ \beta_7    \text{business_close_open_days}+ \beta_8    \text{travel_quarantine_mandate_days}+ \beta_9    \text{stay_at_home_days} \epsilon
\end{equation}
$$

```{r model 3}
model_3_final <- lm(cases_per_100k_at_365d ~ median_transit_change + pop_pct_age_0_24 + population_density + mask_mandate_days + unemployment_benefits_days + increased_weekly_unemployment_insurance_amt_thru_jul31 + business_closed_days_round1 + travel_quarantine_mandate_days + stay_at_home_days, data = final_df)

summary(model_3_final)

# NOT Significant
anova(model_3_final, model_2_final, test = "F")


# Visually Evaluate Model 2b
qplot(model_3_final$residuals,
               geom = "histogram", bins = 20) +
         labs(title = "Histogram of residuals",
              x = "residual")

qplot(model_3_final$fitted, model_3_final$residuals,
            geom = "point") +
         geom_abline(intercept = 0,
                     slope = 0,
                     colour = "red") + 
         geom_smooth(aes(x = model_3_final$fitted,
                         y = model_3_final$residuals),
            colour = 'blue',
            method = "loess", se = TRUE) +
         labs(title = "Plot of residuals vs fitted values",
              x = "fitted value",
              y = "residual")

```


# Regression Table

```{r regression table}
robust_se_1 <- coeftest(model_1_final, vcovHC(model_1_final, type = 'HC3'))[ , "Std. Error"]
robust_se_2 <- coeftest(model_2_final, vcovHC(model_2_final, type = 'HC3'))[ , "Std. Error"]
robust_se_3 <- coeftest(model_3_final, vcovHC(model_3_final, type = 'HC3'))[ , "Std. Error"]

# Print results
stargazer(model_1_final, model_2_final, model_3_final, type = "text",
          se = list(robust_se_1, robust_se_2, robust_se_3),
          title = "Table 1: OLS models for COVID-19 Spread",
          column.sep.width = "3pt")

```


# Plots, Figures, and Tables 

Do the plots, figures and tables that the team has chosen to include successfully move forward the argument that they are making? Has the team chosen the most effective method (a table or a chart) to display their evidence? Is that table or chart the most communicative it could be? Is every plot, figure, and table that is included in the report referenced in the narrative argument?

# Assessment of the CLM. 

Has the team presented a sober assessment of the CLM assumptions that might be problematic for their model? Have they presented their analysis about the consequences of these problems (including random sampling) for the models they estimate? Did they use visual tools or statistical tests, as appropriate? Did they respond appropriately to any violations?

# An Omitted Variables Discussion. 

Did the report miss any important sources of omitted variable bias? Are the estimated directions of bias correct? Was their explanation clear? Is the discussion connected to whether the key effects are real or whether they may be solely an artifact of omitted variable bias?

# Conclusion. 

Does the conclusion address the research question? Does it raise interesting points beyond numerical estimates? Does it place relevant context around the results?

Are there any other errors, faulty logic, unclear or unpersuasive writing, or other elements that leave you less convinced by the conclusions?

# General Notes:

"In principle the SE reflects the degree of uncertainty or the lack of information for getting a 'good' ( that is reliable) estimate of a parameter. Therefore if you keep everything else the same ( eg the same variation in the response, the same number of observations) but you increase the number of separate parameters to be estimated there will be less information per parameter to get the estimate, and hence larger standard error. Precisely what happens will depend on the the degree of variation in the additional X variable that is included and how colinear it is with already included variables."

Known IID violations:
Geo-spatial dependence (states near each other are not independent...physical proximity)
Policy coordination dependence (states near each-other have coordinated policies (like NY/NJ quarantine policies, etc))

Other limitations: 
Mobility data is based on Google-Maps cell phone users. Not everyone has access to a smart phone or uses Google Maps and allows their location to be traced, so this data may not be representative of the population. Additionally, we do not have absolute numbers, only relative change data. 
